{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "34a83867-bbdb-40a3-9226-a9f0b053070c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "Cleaning text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/guillaumelagace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/guillaumelagace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting word frequencies...\n",
      "Exporting to CSV...\n",
      "Data exported to /Users/guillaumelagace/Downloads/Word-Cloud-BudgetQC/BudgetQC2019-2020.csv\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Extract text from the PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    # Preserve apostrophes in words\n",
    "    tokenizer = RegexpTokenizer(r\"\\b[\\w']+\\b\")\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "# Step 2: Clean the text\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    # Use French stopwords\n",
    "    stop_words = set(stopwords.words(\"french\"))\n",
    "\n",
    "    # Use custom tokenizer to preserve apostrophes\n",
    "    words = custom_tokenizer(text)\n",
    "    \n",
    "    # Add additional custom stopwords - self referential keywords offering little to no insight\n",
    "    custom_stopwords = {\n",
    "        \"québec\", \"québécois\", \"québécoise\", \"cinq\", \"dollars\", \"millions\", \"milliards\", \"budget\", \"ans\", \"années\", \"plan\", \"total\", \"notamment\", \n",
    "        \"cette\", \"cet\", \"ainsi\", \"autres\", \"mars\", \"afin\", \"entre\", \"dont\", \"selon\", \"1er\", \"vers\", \"sous\"\n",
    "    }\n",
    "    stop_words.update(custom_stopwords)\n",
    "    \n",
    "\n",
    "    # Define regex patterns\n",
    "    number_pattern = re.compile(r\"^[\\u00b1\\u2013\\u2212+-]?\\d+([.,]\\d+)?$\")  # Matches numbers with ±, −, etc.\n",
    "    year_pattern = re.compile(r\"^[\\u00b1\\u2013\\u2212+-]?\\d{4}([-\\‐‑–—‒﹣－\\u00AD]\\d{4})?|[\\u00b1\\u2013\\u2212+-]?\\d{4}[-‐‑–—‒﹣－\\u00AD]?$\")  # Matches years like 2022, 2022-2023, 2019-\n",
    "    page_number_pattern = re.compile(r\"^[a-zA-Z]\\.\\d+$\")  # Matches page numbers like b.25, c.17\n",
    "    dot_pattern = re.compile(r\"^\\.+$\")  # Matches sequences of dots\n",
    "    bullet_pattern = re.compile(r\"^▪.*$\")  # Matches tokens starting with ▪\n",
    "    hierarchical_number_pattern = re.compile(r\"^\\d+(\\.\\d+)+$\")  # Matches hierarchical numbers like 3.1.1\n",
    "\n",
    "    # New patterns based on examples\n",
    "    range_pattern = re.compile(r\"^\\d{2}[-–—]\\d{2}$\")  # Matches ranges like 06-07 or 17-18\n",
    "    isbn_pattern = re.compile(r\"^\\d{3}-\\d-\\d{3}-\\d{5}-\\d$\")  # Matches ISBN numbers like 978-2-550-83607-0\n",
    "    alphanumeric_pattern = re.compile(r\"^[a-zA-Z]+[-+]?\\d+.*$\")  # Matches alphanumeric sequences like a-13, aa1, aa+\n",
    "    trailing_number_pattern = re.compile(r\"^[^\\d]+(?:-\\d+)?\\d+$\")\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    cleaned_words = [\n",
    "        word.lower() for word in words\n",
    "        if word.lower() not in stop_words\n",
    "        and word not in string.punctuation\n",
    "        and not number_pattern.match(word)\n",
    "        and not year_pattern.match(word)\n",
    "        and not page_number_pattern.match(word)\n",
    "        and not dot_pattern.match(word)\n",
    "        and not bullet_pattern.match(word)\n",
    "        and not hierarchical_number_pattern.match(word)\n",
    "        and not range_pattern.match(word)\n",
    "        and not isbn_pattern.match(word)\n",
    "        and not alphanumeric_pattern.match(word)\n",
    "        and not trailing_number_pattern.match(word)\n",
    "        and len(word) > 2\n",
    "    ]\n",
    "    return cleaned_words\n",
    "\n",
    "\n",
    "# Step 3: Count word frequencies and filter out single occurrences\n",
    "def count_word_frequencies(cleaned_words):\n",
    "    word_counts = {}\n",
    "    for word in cleaned_words:\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "# Step 4: Export to CSV\n",
    "def export_to_csv(word_counts, output_path):\n",
    "    word_freq_df = pd.DataFrame(list(word_counts.items()), columns=[\"Word\", \"Frequency\"])\n",
    "    word_freq_df = word_freq_df.sort_values(by=\"Frequency\", ascending=False)\n",
    "    word_freq_df.to_csv(output_path, index=False, encoding=\"utf-8-sig\", quotechar='\"')\n",
    "    print(f\"Data exported to {output_path}\")\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"/Users/guillaumelagace/Downloads/Word-Cloud-BudgetQC/Budget1920_PlanBudgetaire.pdf\"\n",
    "    output_path = \"/Users/guillaumelagace/Downloads/Word-Cloud-BudgetQC/BudgetQC2019-2020.csv\"\n",
    "\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    print(\"Cleaning text...\")\n",
    "    cleaned_words = clean_text(text)\n",
    "\n",
    "    print(\"Counting word frequencies...\")\n",
    "    word_counts = count_word_frequencies(cleaned_words)\n",
    "\n",
    "    print(\"Exporting to CSV...\")\n",
    "    export_to_csv(word_counts, output_path)\n",
    "\n",
    "    print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
